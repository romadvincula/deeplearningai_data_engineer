{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interacting With Amazon S3 Object Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lab, you'll work with AWS Object Storage and Amazon S3. You will create an S3 bucket, query data from the bucket, and work with object versioning in S3. You will load into the S3 bucket structured data from a CSV file, semi-structured data from a JSON file, and unstructured data from an image file. You will also interact with the S3 bucket through the AWS management console and programmatically using `boto3` (the AWS Software Development Kit (SDK) for Python). \n",
        "\n",
        "*Note*: The lab contains links to external resources. You can always skim through these resources during the lab session, but you're not expected to open and read each link during the lab session. If you'd like to deepen your understanding, you can check the linked resources after you're done with the lab.\n",
        "\n",
        "To open the solution notebook, follow these steps:\n",
        "- Go to the main menu and select `File -> Preferences -> Settings`.\n",
        "- Click on `Text Editor` on the left, then scroll down to the `Files: Exclude` section.\n",
        "- Remove the line `**/C2_W1_Lab_3_S3_Solution.ipynb`. The file will now appear in the explorer.\n",
        "- You can close the `Settings` tab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "- [ 1 - Import Packages](#1)\n",
        "- [ 2 - Explore the Dataset](#2)\n",
        "- [ 3 - Create an S3 Bucket](#3)\n",
        "  - [ Exercise 1](#ex01)\n",
        "- [ 4 - Upload and Query Data](#4)\n",
        "  - [ 4.1 - Structured Data](#4-1)\n",
        "  - [ 4.2 - Semi-Structured Data](#4-2)\n",
        "    - [ Exercise 2](#ex02)\n",
        "    - [ Exercise 3](#ex03)\n",
        "  - [ 4.3 - Unstructured Data](#4-3)\n",
        "    - [ Exercise 4](#ex04)\n",
        "- [ 5 - Delete the Bucket](#5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='1'></a>\n",
        "## 1 - Import Packages\n",
        "\n",
        "Let's import the packages required for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "from typing import Any, Dict\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2'></a>\n",
        "## 2 - Explore the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lab, you are provided with three data files that you can find in the `data` folder. Here's the structure of the `data` folder:\n",
        "\n",
        "```bash\n",
        ".\n",
        "└── data/\n",
        "    ├── csv/\n",
        "         └── ratings_ml_training_dataset.csv\n",
        "    ├── images/\n",
        "    |    ├── v1/\n",
        "    |    |    └── AWS-Logo.png\n",
        "    |    └── v2/ \n",
        "    |    |    └── AWS-Logo.png\n",
        "    └── json/\n",
        "         └── delivery-stream-one-record.json\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that there are three subfolders (csv, images and json). Each subfolder contains a different data format: \n",
        "- the csv subfolder contains structured data, stored in a `.csv` file. This data consists of the training dataset that was used in the Week 4 lab of Course 1 to train the recommender system model;\n",
        "- the json subfolder contains semi-structured data stored in a `.json` file;\n",
        "- the images subfolder contains unstructured data, that consists of two versions of AWS logo.\n",
        "\n",
        "You will upload these different types of data in an S3 bucket that you will create in this lab. Here's a quick summary of AWS object storage terminology:\n",
        "- A [bucket](\n",
        "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html) is a container for objects stored in Amazon S3.\n",
        "- An object is a file and any metadata that describes that file. It has a unique identifier, also known as the object key.\n",
        "Object storage allows the storage of any object; you can store not only structured but also unstructured and semi-structured data. \n",
        "\n",
        "To upload your data to Amazon S3, you need first to create an S3 bucket in one of the AWS Regions. And this is what you're going to do programmatically in the next exercise. But before that, check that there are not S3 buckets in your account. Run the following code to get the URL to the AWS console."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Note*: For security reasons, the URL to access the AWS console will expire every 15 minutes, but any AWS resources you created will remain available for the 2 hour period. If you need to access the console after 15 minutes, please rerun this code cell to obtain a new active link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a href=\"https://signin.aws.amazon.com/federation?Action=login&SigninToken=cX1ea5v1Se8q6kyFW0IqBjCItTJrkKsYGjudnMo92EKvaxGsFjIN4uKDAwxI0Jm2vAGVVv_sXSPY_eKXjO0-6l9vyihwZd8L1PKG_Yj3XWk7Wsi6K5rfgdur8iWE1Jz99cexrjpbN-5FVZR_PMKdqIhj0CnK3UY2ylqRtptIRh4HXia-j2NsMv4lI6oJOqIWA7QOcd6MJpeUjcaWyySbdlyaGYSn3FdD7olc82MtF2ZBClB4HownNgIg8SsJpTf_9PIu8DrsvNAgyYASs3CR99OoyNfJgpW7zTCDfKCqlhdQrsb_RPS5r4V8HnfF3UpEXvQj5qVodn7lcyAKejiXYmQBplZr8ZbEnc0nk4x8nY9wJX2KvU9-EuioszpNZUh7AJZCR_y_4h3HSfwOOjVpxV7C7o3056oPI74cv7zcLFPF3H_uVbFljK1NUZ4bO1x4EW0bfxnjeZWzfK3hFCOwu0aBnao81Jq4x4-MRYpLMHy2jX1ir4L92hDiOzuGLlhNf0m5t__qnKOBYHhDLbpd34o67Fz5DiIviQaYLIjimVLk-Wjd0WiV6AYQ-xs0eplNLxzpu5Gqgl7XmjWdilFX_wgfKUj-GwyrNe6ZgdCyvM2VIpWQ0wVgQjOKy1jTOiaruL7WeqVBkHpuYQEP5O9QUSzxfCRDZh-4Msx8RoXcXLCv8-pNkwgI4LqNROZFTGhjU8cepmr-9GJbELwvOS6bSH4a3fx72UjAMXNezT7bvzOYwq74R-sxwHX1SMxPy_HTpX9_vm-v2yv0YJlia32nzinCO1XxYD_vi3A0yPXTaWoniBoS0j3ELAYYvgZMVEOkX-THA_hpvUIH3MKNffhvxH3kTJIb1QpbVJOU7z9Ku0uQG0OW7q8_NgSt5xe92aCwzChaGQP9hOJ2t8RhAE5Pbjdeh8dfn4pgPBi941HNv05lIEz3OZ_k2bsEfdcSAmKjpdPINT7qVRYTyIBgejsfJfdi5mWa5Z4tNwbtkBaQsj2zuhH-MxGTcV2wJbLjYBGX-McefbEepU1qsX1yMJCpffJeV5ISlw9AJ3ti_PXt5ywLkWoLf8ic4PRYjrCjX-UwM7R1oSO5HDhQcPl2l-0BuJ5XU2Ng2b0pBooLstSWZ0XCWd1R51BnaOqlZWOFPEyloEUXmrZE7VKX9Shr9EsfMXl6jc_7ZVZZT0dLXn0h2qUxAWtlo-gNM96vAC6tWwZlZ5MWelBE9aL3ejVnN9PAiGxJ4YwdFf19Qtl2Oxn5CBrjZTIJb5hRrDzMC8snLgt38mGqpoGHvRYKpf81XUEqIhnkeMjLSGiu1eQ2verEEYmf_KGVMAi7-KW0X3Yvg-VGY6ITATuzu7Cqjrh3_xK5uz1lyuzpdhm0cGsFkkNagkEyxpphrf7bqfZtF5xlZ4YQ4fEgbwj7lxz6_r1rkvlYN94Nh174S7tH_jGRcmBJvo2JHZaII8FbkMA&Issuer=https%3A%2F%2Fapi.vocareum.com&Destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fregion%3Dus-east-1\" target=\"_blank\">GO TO AWS CONSOLE</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('../.aws/aws_console_url', 'r') as file:\n",
        "    aws_url = file.read().strip()\n",
        "\n",
        "HTML(f'<a href=\"{aws_url}\" target=\"_blank\">GO TO AWS CONSOLE</a>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now click on the link above to access the AWS console and search for **S3**. You should see that there are no buckets created yet in your account.\n",
        "\n",
        "*Note:* If you see the window like in the following printscreen, click on **logout** link, close the window and click on console link again.\n",
        "\n",
        "![AWSLogout](images/AWSLogout.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='3'></a>\n",
        "## 3 - Create an S3 Bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "To create an S3 bucket, you need to specify the bucket name and the region for your bucket. In this lab, the region is set to `us-east-1`. Regarding the name of the bucket, Bucket names should be globally unique to avoid collisions with buckets that other learners could be creating or working with at the same time. To ensure the uniqueness of the bucket name, you will use your Vocareum's AWS Account ID to include it in the bucket name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "AWS_ACCOUNT_ID = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], capture_output=True, text=True).stdout.strip()\n",
        "BUCKET_NAME = f'de-c2w1lab3-{AWS_ACCOUNT_ID}'\n",
        "AWS_DEFAULT_REGION = 'us-east-1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To programmatically create the bucket in Python using boto3, you can use the `S3` method [`create_bucket()`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_bucket.html) which requires a Client object (as explained in the DynamoDB lab).\n",
        "\n",
        "The following function `create_s3_bucket()` consists of the steps needed to create the S3 bucket (instantiating a Client object and then calling the method `create_bucket()`). The function takes in as input the bucket name and the region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_s3_bucket(bucket_name:str , region: str):\n",
        "    \n",
        "    # Create an S3 client\n",
        "    s3_client = boto3.client('s3', region_name=region)\n",
        "\n",
        "    # Create the S3 bucket\n",
        "    try:\n",
        "        s3_client.create_bucket(Bucket=bucket_name)\n",
        "        print(f\"S3 bucket '{bucket_name}' created successfully in region '{region}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S3 bucket 'de-c2w1lab3-269225172703' created successfully in region 'us-east-1'.\n"
          ]
        }
      ],
      "source": [
        "create_s3_bucket(bucket_name=BUCKET_NAME, region=AWS_DEFAULT_REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can check that the bucket has been created using the `aws cli` tool. To list the buckets created in your account, you can use the following command:\n",
        "`aws s3 ls`\n",
        "\n",
        "You can run the command in the terminal or you can run it in this notebook but you need to add an exclamation mark `!` at the beginning of the command. This allows you to run shell commands in a code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 23:10:50 de-c2w1lab3-269225172703\n"
          ]
        }
      ],
      "source": [
        "!aws s3 ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To list the objects stored inside a bucket, you can use the command `aws s3 ls <your-bucket-name>`. If you now run this command, no result will be shown since the bucket is still empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "!aws s3 ls $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also inspect the S3 bucket in the AWS Console. Search for **S3**. You will see the bucket with the name you provided. You can check that the bucket is empty by simply clicking on it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4'></a>\n",
        "## 4 - Upload and Query Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4-1'></a>\n",
        "### 4.1 - Structured Data\n",
        "\n",
        "In this section of the lab, you will upload structured data to the S3 bucket and then query it. \n",
        "\n",
        "**Upload the CSV file**\n",
        "\n",
        "Check first the file `data/csv/ratings_ml_training_dataset.csv`. Each row in this dataset consists of the details of a product that was purchased by a given user. The row also contains the user details and what ratings they provided to that product (the same dataset was used in the Week 4 lab of Course 1 to train the recommender system). Here's the structure of this table:\n",
        "\n",
        "![schema_after_ETL](images/schema_after_ETL.png \"Ratings dataset\")\n",
        "\n",
        "To programmatically upload this CSV file to the bucket, you can use the S3 method [upload_file()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/upload_file.html). This method expects three arguments: the path of the source file that you want to upload (Filename), the name of the bucket to upload to (Bucket) and the object key or name (Key). The last argument specifies how you want to label the uploaded object or file within the bucket, this name should uniquely identify the uploaded object.\n",
        "\n",
        "The following function `upload_file_to_s3()` consists of the steps needed to upload the file to the S3 bucket (instantiating a Client object and then calling the method `upload_file()`). The function takes in as input the path to the local file to upload, the bucket name, and the object key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_file_to_s3(local_file_path: str, bucket_name: str, object_key: str) -> None:\n",
        "    \"\"\"Uploads a local file to S3 using boto3\n",
        "\n",
        "    Args:\n",
        "        local_file_path (str): Local file path\n",
        "        BUCKET_NAME (str): Bucket name\n",
        "        object_key (str): the key name, which should uniquely identifies the uploaded object in the bucket\n",
        "    \"\"\"\n",
        "    # Create an S3 client\n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    # Upload the file to S3\n",
        "    try:\n",
        "        s3_client.upload_file(local_file_path, bucket_name, object_key)\n",
        "        print(f\"File {local_file_path} uploaded to s3://{bucket_name}/{object_key} successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file to S3: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File data/csv/ratings_ml_training_dataset.csv uploaded to s3://de-c2w1lab3-269225172703/csv/ratings_ml_training_dataset.csv successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define the local file path, and object key\n",
        "local_file_path = 'data/csv/ratings_ml_training_dataset.csv'\n",
        "object_key = 'csv/ratings_ml_training_dataset.csv'\n",
        "\n",
        "# Upload the file to S3\n",
        "upload_file_to_s3(local_file_path, BUCKET_NAME, object_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can verify that the file is in the bucket either by checking the content through the AWS console or programmatically also using `boto3` or even the `aws cli`. Go to the AWS console and check that there's a new folder in your bucket that contains the csv file you just uploaded. \n",
        "\n",
        "*Note*: remember from the lecture that object storage has a flat structure. When you use the delimiter `/` in the object name or key, like in this example: `object_key = 'csv/ratings_ml_training_dataset.csv'`, you're including a key name prefix that is used by S3 to group objects inside the bucket. The console uses the terminology `folder` because this grouping of objects can be analogous to a folder in a regular file system. You can learn more about object keys [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html). \n",
        "\n",
        "If you run the next command either in the terminal, or the notebook cell, you can also check that the file you uploaded is there. Remember that if you run it in a terminal, you have to omit the exclamation mark at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 23:29:47     274459 ratings_ml_training_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "!aws s3 ls $BUCKET_NAME/csv/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Query the Data**\n",
        "\n",
        "You can query the data from your csv files by using AWS Athena. This is something you have done in previous labs like in C1W2 Assignment and you will continuing doing it in future labs. In general, AWS Athena allows to query data in different formats, like parquet, tsv, csv, etc. As you will continue querying data with Athena in future labs, for this particular lab you will only be pointed to some documentation that you can read to have some insights about it:\n",
        "\n",
        "* [Querying data from the AWS console](https://builtin.com/articles/aws-architecture-athena-query-csv-table-stored-s3)\n",
        "* [Querying data from multiple sources at AWS forum](https://repost.aws/questions/QUeZq3d77YQ8-9EPtDDBe6RQ/query-data-from-multiple-sources-in-s3-on-athena)\n",
        "* [Usage of delimiter at AWS documentation](https://docs.aws.amazon.com/athena/latest/ug/lazy-simple-serde.html)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4-2'></a>\n",
        "### 4.2 - Semi-Structured Data\n",
        "\n",
        "Now, you will work with semi-structured data, in particular with a JSON file. You will upload the file located at `data/json/delivery-stream-one-record.json`. This file consists of the data obtained from the transformations done to the streaming data in the Week 4 lab of Course 1. You can open it to check its structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "Complete the code below to upload the file located at `data/json/delivery-stream-one-record.json` to the S3 bucket using the same function used for the CSV file above. But now in the S3 bucket, point to a new folder `json`, giving the same name to the file (i.e., the object key should start with \"json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File data/json/delivery-stream-one-record.json uploaded to s3://de-c2w1lab3-269225172703/json/delivery-stream-one-record.json successfully.\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ### (~ 3 lines of code)\n",
        "# Define the local file path, and S3 key\n",
        "local_file_path_json = 'data/json/delivery-stream-one-record.json'\n",
        "object_key_json = 'json/delivery-stream-one-record.json'\n",
        "\n",
        "# Upload the file to S3\n",
        "upload_file_to_s3(local_file_path_json, BUCKET_NAME, object_key_json)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "Verify again that the file has been uploaded using the `aws cli` tool. Complete the command pointing to the corresponding folder where the JSON file has been uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 23:43:07       1784 delivery-stream-one-record.json\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ### (~ 1 line of code)\n",
        "!aws s3 ls $BUCKET_NAME/json/\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also visually check in the AWS console that the JSON file has been created in the bucket. Now, instead of querying the JSON file, you will download it using the `S3` [download_file()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/download_file.html) method, which is called in the following provided function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_object_from_s3(bucket_name: str, object_key: str, local_file_path: str) -> None:\n",
        "    \"\"\"Downloads object from S3 using boto3\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Bucket name\n",
        "        object_key (str): Object key in S3.\n",
        "        local_file_path (str): Path in the local file system to put the downloaded object.\n",
        "    \"\"\"\n",
        "    # Create an S3 client\n",
        "    s3_client = boto3.client('s3')\n",
        "    \n",
        "    try:\n",
        "        # Download the file to a local directory\n",
        "        s3_client.download_file(bucket_name, object_key, local_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading or printing JSON file: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to create a `downloads` folder in your local file system, and then, call the function `download_object_from_s3` to download the JSON file from your S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir 'downloads'\n",
        "local_file_path = './downloads/delivery-stream-one-record.json'\n",
        "\n",
        "download_object_from_s3(bucket_name=BUCKET_NAME, object_key=object_key_json, local_file_path=local_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the file has been downloaded, you can read its content from the local file system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'session_id': '45ba9939-df9c-4ca9-a11d-2786b63983fd', 'customer_number': 478, 'city': 'Pasadena', 'country': 'USA', 'credit_limit': 59811, 'browse_history': [{'product_code': 'S50_1514', 'quantity': '1', 'in_shopping_cart': True}, {'product_code': 'S18_1342', 'quantity': '1', 'in_shopping_cart': False}, {'product_code': 'S700_1938', 'quantity': '7', 'in_shopping_cart': True}], 'recommended_items': [{'id': 'S12_1108', 'score': 0.7991661648934496}, {'id': 'S18_2325', 'score': 0.7757266230678397}, {'id': 'S18_1589', 'score': 0.6731320331167012}, {'id': 'S24_3856', 'score': 0.6657197007471041}, {'id': 'S10_1949', 'score': 0.6148422263794373}], 'similar_items': {'product_code': 'S50_1514', 'similar_items': [{'id': 'S18_2238', 'distance': 0.7111176212606649}, {'id': 'S12_1099', 'distance': 0.8128645620864546}, {'id': 'S24_3969', 'distance': 0.8139055007312936}, {'id': 'S18_3259', 'distance': 0.8341333315091493}, {'id': 'S18_2949', 'distance': 0.8714841220893687}]}}\n"
          ]
        }
      ],
      "source": [
        "with open(local_file_path, 'r') as file:    \n",
        "    json_content = json.loads(file.read())\n",
        "    print(json_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can work with this particular object if you need to make any transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4-3'></a>\n",
        "### 4.3 - Unstructured Data\n",
        "\n",
        "Finally, you will work with unstructured data. You will upload an image to the bucket and, this time, download it from a browser (to show you the various ways you can download objects from an S3 bucket). By default, an S3 bucket and its objects are private. To be able to download S3 objects from a browser, you will have to make some modifications to the bucket to make some of its objects available for public reading. \n",
        "\n",
        "First, you need to configure the bucket to accept public policies and public Access Control Lists (ACLs). To do so, you will use the method `S3 put_public_access_block`. To understand what this method expects as arguments, check the following [documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_public_access_block.html).\n",
        "\n",
        "Run the following two cells to change the access configuration of the S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def s3_public_access_setup(bucket_name: str, public_access_block_configuration: Dict[str, Any]) -> None:\n",
        "    \"\"\"Sets public access configuration for S3 bucket\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Bucket name\n",
        "        public_access_block_configuration (Dict[str, Any]): Configuration for public access\n",
        "    \"\"\"\n",
        "    \n",
        "    s3_client = boto3.client('s3')\n",
        "    \n",
        "    # Update the bucket's public access settings\n",
        "    s3_client.put_public_access_block(\n",
        "        Bucket=bucket_name,\n",
        "        PublicAccessBlockConfiguration=public_access_block_configuration\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the public access settings  \n",
        "public_access_configuration = {\n",
        "    'BlockPublicAcls': False,\n",
        "    'IgnorePublicAcls': False,\n",
        "    'BlockPublicPolicy': False,\n",
        "    'RestrictPublicBuckets': False\n",
        "}\n",
        "\n",
        "s3_public_access_setup(bucket_name=BUCKET_NAME, \n",
        "                       public_access_block_configuration=public_access_configuration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You just modified the bucket so that it now accepts public access rules to its objects. You'll now attach a policy to the bucket to allow anyone on the internet to have reading access to the objects whose key starts with `images/`. (\"A policy is an object in AWS that, when associated with an identity or resource, defines their permissions\", [reference](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_access-management.html). You will learn more about policies in the next lesson or you can check the documentation [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html)).\n",
        "\n",
        "To attach the mentioned policy to the S3 bucket, you'll use the `S3` [put_bucket_policy()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_bucket_policy.html) method, define the details of the policy and pass in the policy to `S3 put_bucket_policy()`. Run the following three cells to attach the appropriate policy to the S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def s3_put_bucket_policy(bucket_name: str, policy: Dict[str, Any]) -> None:\n",
        "    \"\"\"Allows to put bucket policies\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Bucket name\n",
        "        policy (Dict[str, Any]): Bucket policy\n",
        "    \"\"\"\n",
        "    \n",
        "    s3_client = boto3.client('s3')\n",
        "    response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = { \n",
        "    \"Version\": \"2012-10-17\", \n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Principal\": \"*\",\n",
        "            \"Action\": \"s3:GetObject\",\n",
        "            \"Resource\": f\"arn:aws:s3:::{BUCKET_NAME}/images/*\"\n",
        "        }\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This policy allows anyone (`\"Principal\": \"*\"`) to use the method `S3 GetObject` on `{BUCKET_NAME}/images/`, i.e., to retrieve objects stored in your s3 bucket and whose key/name starts with `images/`. You can learn more about such policy [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#access_policies-json)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': '5BYSMA8FVWBBXZZ0', 'HostId': 'BKG/LUCyDTmNI/lbSkXvnBOqQ2d2anjndrV1X7bz5aLs+L0EUmd/vF4rXxCsko52gFSrh/q2dQQ=', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': 'BKG/LUCyDTmNI/lbSkXvnBOqQ2d2anjndrV1X7bz5aLs+L0EUmd/vF4rXxCsko52gFSrh/q2dQQ=', 'x-amz-request-id': '5BYSMA8FVWBBXZZ0', 'date': 'Thu, 10 Apr 2025 23:54:33 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = s3_put_bucket_policy(bucket_name=BUCKET_NAME, policy=policy) \n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex04'></a>\n",
        "### Exercise 4\n",
        "\n",
        "Now, let's upload the image located at `data/images/v1/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File data/images/v1/AWS-Logo.png uploaded to s3://de-c2w1lab3-269225172703/images/AWS-Logo.png successfully.\n"
          ]
        }
      ],
      "source": [
        "local_file_path_image_v1 = 'data/images/v1/AWS-Logo.png'\n",
        "object_key_image = 'images/AWS-Logo.png'\n",
        "\n",
        "upload_file_to_s3(local_file_path_image_v1, BUCKET_NAME, object_key_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check that the image has been uploaded. Complete the command below. Remember to point to the correct folder in S3 to list only the file you just uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 23:55:16      28107 AWS-Logo.png\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ### (~ 1 line of code)\n",
        "!aws s3 ls $BUCKET_NAME/images/\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Go to the AWS console, and search for **S3**. In your bucket, click on images and click on the name of the image you just uploaded. You can see an option called `Object URL`. If you copy it and paste it into a new browser's tab, you should be able to download the file.\n",
        "\n",
        "<img src=\"images/object_url.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the Bucket Versioning works. You need first to enable this feature in your bucket by calling the method `S3` [put_bucket_versioning()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_bucket_versioning.html) and switching on versioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_bucket_versioning(bucket_name: str, versioning_config: Dict[str, str]) -> Dict[Any, Any]:\n",
        "    \n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    # Enable bucket versioning\n",
        "    response = s3_client.put_bucket_versioning(\n",
        "        Bucket=bucket_name,\n",
        "        VersioningConfiguration=versioning_config\n",
        "    )\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': '9STY0FFPWJW4S13N', 'HostId': 'zT0BcOqXn2JRg6TG/ajoxpa4jAHEUuaa1eGtbDEliWj0SY9s2GtMsRJAtxkiLky3qIgdYML6Pp0=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'zT0BcOqXn2JRg6TG/ajoxpa4jAHEUuaa1eGtbDEliWj0SY9s2GtMsRJAtxkiLky3qIgdYML6Pp0=', 'x-amz-request-id': '9STY0FFPWJW4S13N', 'date': 'Thu, 10 Apr 2025 23:58:06 GMT', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ],
      "source": [
        "versioning_config = {'Status': 'Enabled'}\n",
        "\n",
        "response = configure_bucket_versioning(bucket_name=BUCKET_NAME, \n",
        "                                       versioning_config=versioning_config)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now upload the second version of the AWS logo located at `data/images/v2/AWS-Logo.png` and use the same object key or name you used for the previous image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File data/images/v2/AWS-Logo.png uploaded to s3://de-c2w1lab3-269225172703/images/AWS-Logo.png successfully.\n"
          ]
        }
      ],
      "source": [
        "local_file_path_image_v2 = 'data/images/v2/AWS-Logo.png'\n",
        "object_key_image = 'images/AWS-Logo.png'\n",
        "\n",
        "upload_file_to_s3(local_file_path_image_v2, BUCKET_NAME, object_key_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, you have listed the content of the bucket using the `aws cli` tool. You can also list the content in Python using `S3` [list_objects_v2()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_objects_v2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_objects_in_folder(bucket_name: str, prefix_key: str):\n",
        "    # Create an S3 client\n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    # Use list_objects_v2 to list objects in the specified folder\n",
        "    response = s3_client.list_objects_v2(\n",
        "        Bucket=bucket_name,\n",
        "        Prefix=prefix_key\n",
        "    )\n",
        "\n",
        "    # Check if objects were found\n",
        "    if 'Contents' in response:\n",
        "        # Print each object's key\n",
        "        print(\"Objects with a key that starts with '{}':\".format(prefix_key))\n",
        "        for obj in response['Contents']:\n",
        "            print(obj['Key'])\n",
        "    else:\n",
        "        print(\"No objects found in folder '{}'.\".format(prefix_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Objects with a key that starts with 'images':\n",
            "images/AWS-Logo.png\n"
          ]
        }
      ],
      "source": [
        "list_objects_in_folder(bucket_name=BUCKET_NAME, prefix_key='images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method only shows you the files whose key starts with a particular prefix, but you cannot see anything about their versions. For that, let's use the [`S3 list_object_versions()`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_object_versions.html) method instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object Key: images/AWS-Logo.png\n",
            "Object Version Id: qNHosD1ZSPJ9wmC8.WVcyMq.r0LRvqQl\n",
            "Is Latest: True\n",
            "Last Modified: 2025-04-10 23:58:34+00:00\n",
            "\n",
            "Object Key: images/AWS-Logo.png\n",
            "Object Version Id: null\n",
            "Is Latest: False\n",
            "Last Modified: 2025-04-10 23:55:16+00:00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def list_object_versions(bucket_name: str, prefix_key: str) -> None:\n",
        "    # Create an S3 client\n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    # List object versions\n",
        "    response = s3_client.list_object_versions(Bucket=bucket_name, Prefix=prefix_key)\n",
        "\n",
        "    # Process the response to get object versions\n",
        "    for version in response.get('Versions', []):\n",
        "        print(\"Object Key:\", version['Key'])\n",
        "        print(\"Object Version Id:\", version['VersionId'])\n",
        "        print(\"Is Latest:\", version['IsLatest'])\n",
        "        print(\"Last Modified:\", version['LastModified'])\n",
        "        print()\n",
        "\n",
        "list_object_versions(bucket_name=BUCKET_NAME, prefix_key='images/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, go back to the S3 bucket in the AWS console and search for the file you just uploaded. Get its Object URL to download the new version of the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5'></a>\n",
        "## 5 - Delete the Bucket\n",
        "\n",
        "To delete the bucket, you need to make sure it is empty before the deletion process. And for that, there are two methods that you can use: `S3` [delete_object()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_object.html) and `S3` [delete_bucket()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_bucket.html).\n",
        "\n",
        "In the next cell, you are provided with a function that makes use of `S3 delete_object()` and `S3 delete_bucket()`. This function takes as an input the boolean parameter`delete_objects`; this boolean parameter is used to indicate if the bucket is empty or not. If the bucket contains objects, then the function first deletes the objects and then the bucket. Otherwise, the function directly deletes the bucket. Note that you need to delete all object versions. The deletion of versions is necessary only if you have enabled Bucket Versioning. Note that the function also removes delete markers. These are placeholders that are created after you delete objects in a versioning-enabled bucket. You can learn more about them [here](https://www.learnaws.org/2022/10/04/aws-s3-delete-marker/#what-is-an-aws-s3-delete-marker).\n",
        "\n",
        "**Note:** It is important to take into account that when you are working with S3 buckets in real life and production environments, you SHOULD NOT delete them or delete the objects within them unless you are completely sure about what you are doing. Make sure that the bucket/objects are not used anymore by any upstream or downstream process. This is something you should do with caution and after talking with bucket/object owners, stakeholders, and other process owners who may depend on the information hosted in that bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ResponseMetadata': {'RequestId': 'J8H5AZGF76P252KB', 'HostId': 'wUttFvaBNy6wyg9VUU8sdLY4oiXPi4V5beAAGtyrmxdrvPnz2aq0pFvbSXooqmU9GwpQu6tWS3Fj2KeWmwEBRa6ljvdGcQFP', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': 'wUttFvaBNy6wyg9VUU8sdLY4oiXPi4V5beAAGtyrmxdrvPnz2aq0pFvbSXooqmU9GwpQu6tWS3Fj2KeWmwEBRa6ljvdGcQFP', 'x-amz-request-id': 'J8H5AZGF76P252KB', 'date': 'Fri, 11 Apr 2025 00:08:32 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n"
          ]
        }
      ],
      "source": [
        "def s3_delete_bucket(bucket_name: str, delete_objects: bool) -> Dict[Any, Any]:\n",
        "    s3_client = boto3.client('s3')\n",
        "    \n",
        "    if delete_objects:\n",
        "        # List all versions of all objects in the bucket\n",
        "        response = s3_client.list_object_versions(Bucket=bucket_name)\n",
        "        \n",
        "        # Delete all object versions\n",
        "        for version in response.get('Versions', []):\n",
        "            key = version['Key']\n",
        "            version_id = version['VersionId']\n",
        "            s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=version_id)\n",
        "        \n",
        "        # Delete all delete markers\n",
        "        for delete_marker in response.get('DeleteMarkers', []):\n",
        "            key = delete_marker['Key']\n",
        "            version_id = delete_marker['VersionId']\n",
        "            s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=version_id)        \n",
        "    \n",
        "    # Delete the bucket\n",
        "    response = s3_client.delete_bucket(\n",
        "        Bucket=bucket_name\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "response = s3_delete_bucket(bucket_name=BUCKET_NAME, delete_objects=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, check that the bucket no longer exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "!aws s3 ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well done! You learned how to work with Amazon S3, such as how to create S3 buckets, upload files to S3 buckets, and understand some of its features like versioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbconvert in /home/coder/miniconda/lib/python3.12/site-packages (7.16.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (0.10.1)\n",
            "Requirement already satisfied: nbformat>=5.7 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (5.10.4)\n",
            "Requirement already satisfied: packaging in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (2.15.1)\n",
            "Requirement already satisfied: tinycss2 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (1.4.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /home/coder/miniconda/lib/python3.12/site-packages (from nbconvert) (5.14.3)\n",
            "Requirement already satisfied: webencodings in /home/coder/miniconda/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /home/coder/miniconda/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert) (3.10.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /home/coder/miniconda/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /home/coder/miniconda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /home/coder/miniconda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/coder/miniconda/lib/python3.12/site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/coder/miniconda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/coder/miniconda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/coder/miniconda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/coder/miniconda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.22.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/coder/miniconda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /home/coder/miniconda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /home/coder/miniconda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/coder/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.16.0)\n",
            "Collecting pyppeteer\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /home/coder/miniconda/lib/python3.12/site-packages (from pyppeteer) (2024.8.30)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /home/coder/miniconda/lib/python3.12/site-packages (from pyppeteer) (4.66.5)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /home/coder/miniconda/lib/python3.12/site-packages (from pyppeteer) (1.26.6)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting zipp>=3.20 (from importlib-metadata>=1.4->pyppeteer)\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/coder/miniconda/lib/python3.12/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.12.2)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: websockets\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=98716 sha256=8933ee97d12abdca6573e578e740fc02043b8fd39fc876639c593bafdbc0d4fe\n",
            "  Stored in directory: /home/coder/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built websockets\n",
            "Installing collected packages: appdirs, zipp, websockets, pyee, importlib-metadata, pyppeteer\n",
            "Successfully installed appdirs-1.4.4 importlib-metadata-8.6.1 pyee-11.1.1 pyppeteer-2.0.0 websockets-10.4 zipp-3.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nbconvert\n",
        "!pip install pyppeteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pip 24.2 from /home/coder/miniconda/lib/python3.12/site-packages/pip (python 3.12)\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --to pdf your_notebook.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f7c385fa1e05902489312e0f26958bd15563da08ffbc61abb00afd1f64e2ab3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
